---
layout: post
title: "Integrating Rspamd with GPT"
categories: misc
---

## Preface

Historically, our only text classification method has been Bayes, a powerful statistical method that performs well with sufficient training. However, Bayes has its limitations:

* It requires thorough and well-balanced training
* It cannot work with low confidence levels, especially when dealing with a wide variety of spam

Large Language Models (LLMs) offer promising solutions to these challenges. These models can perform deep introspection with some sort of contextual "understanding". However, their high computational demands (typically requiring GPUs) make scanning all emails impractical. Separating LLM execution from the scanning engine mitigates resource competition.

## Rspamd GPT plugin

In Rspamd 3.9, I have tried to integrate the OpenAI GPT API for spam filtering and assess its usefulness. Here are some basic ideas behind this plugin:

* The selected displayed text part is extracted and submitted to the GPT API for spam probability assessment
* Additional message details such as Subject, Display From, and URLs are also included in the assessment
* Then we ask GPT to provide results in JSON format since human-readable GPT output cannot be parsed (in general)
* Some specific symbols (`BAYES_SPAM`, `FUZZY_DENIED`, `REPLY`, etc.) are excluded from the GPT scan
* Obvious spam and ham are also excluded from the GPT evaluation

The former two points reduce the GPT workload for something that is already known, where GPT cannot add any value in the evaluation. We also use GPT as one of the classifiers, meaning that we do not rely solely on GPT evaluation.

## Evaluation results

To evaluate the performance of the GPT based classifier, we have created an utility `rspamadm classifier_test` that can evaluate both supervised and unsupervised classifiers.

* It splits spam and ham samples into two sets: training and validation
* For supervised classifiers it uses training set to learn classifier
* Then for both supervised and unsupervised classifiers it uses validation set to check the performace of the classifier

For example, Bayes engine, learned on a decent training corpus has the following result:

~~~
$ rspamadm classifier_test --ham /ham --spam /spam --cv-fraction 0.3

Spam: 348 train files, 815 cv files; ham: 754 train files, 1762 cv files
Start learn spam, 348 messages, 10 connections
Start learn ham, 754 messages, 10 connections
Learning done: 348 spam messages in 1.61 seconds, 754 ham messages in 3.88 seconds
Start cross validation, 2577 messages, 10 connections
Metric               Value
------------------------------
True Positives       735
False Positives      22
True Negatives       1717
False Negatives      49
Accuracy             0.97
Precision            0.97
Recall               0.94
F1 Score             0.95
Classified (%)       97.90
Elapsed time (seconds) 12.71
~~~

These results are quite good, but they assume that the classifier is properly and decently trained. For the case when we have a fresh system or high variability of emails it might be close to impossible to gather somehow decent statistics.

GPT engine can be treated in this case as unsupervised learning algorithm: we don't really 'learn' it on emails, we assume that LLM models have enough "understanding" of the language to distinguish spam and ham. We also do not provide raw emails to them, only text data. 

So here are some results from different models:

### OpenAI GPT3.5-turbo

~~~
Metric               Value
------------------------------
True Positives       129
False Positives      35
True Negatives       263
False Negatives      69
Accuracy             0.79
Precision            0.79
Recall               0.65
F1 Score             0.71
Classified (%)       95.20
Elapsed time (seconds) 318.91
~~~

This model is quite cheap and can be used as the baseline. The results were obtained from the corpus with low quality samples, so there are quite a lot of FP/FN in the results.

### OpenAI GPT4-o

~~~
Metric               Value
------------------------------
True Positives       178
False Positives      25
True Negatives       257
False Negatives      9
Accuracy             0.93
Precision            0.88
Recall               0.95
F1 Score             0.91
Classified (%)       90.02
Elapsed time (seconds) 279.08
~~~

This model is expensive and very advanced but it can still be used, for example, for the low traffic personal email. The results were also obtained from the corpus with low quality samples, but the error rate is significantly lower than with GPT3.5.

## Plugin design

TODO

## Pricing considerations

TBD

## Conclusions

TBD
